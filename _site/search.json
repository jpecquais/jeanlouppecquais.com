[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Comprendre les différents stratégies de spatialisation sonore\n\n\nQue dois-je écouter sur ce site web ?\n\n\n\nspatialisation\n\n\n\n\n\n\n\n\n\n3 juil. 2025\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\nMixage Ambisonique dans REAPER - Introduction\n\n\nUne Alternative au Dolby Atmos\n\n\n\nguitar\n\n\n\n\n\n\n\n\n\n4 déc. 2024\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\nPt.II - Mise en place du dispositif\n\n\n\ndsp\n\n\nguitar\n\n\ndeconvolution\n\n\n\n\n\n\n\n\n\n3 nov. 2024\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\nPt.I - Enoncé du problème\n\n\n\ndsp\n\n\nguitar\n\n\n\n\n\n\n\n\n\n16 oct. 2024\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jean-Loup Pecquais",
    "section": "",
    "text": "Jean-Loup Pecquais est ingénieur du son, spécialisé dans le son immersif, l’informatique audio et le traitement du signal sonore. Il travaille actuellement comme développeur et ingénieur applicatif chez Harman, et enseigne à l’ENS Louis Lumière.\nIl est également guitariste et membre des groupes Outwards, Pompéi et Burger Bang.\n\n\n\n\n\n\n\n\n\nTitre\n\n\nDate\n\n\n\n\n\n\n\n\n\nPompéi\n\n\n14 mars 2025\n\n\n\n\n\nAucun article correspondant\n\n\n\n\n\n\n\n\n\n\n\n\nTitre\n\n\nDate\n\n\n\n\n\n\n\n\n\nComprendre les différents stratégies de spatialisation sonore\n\n\n3 juil. 2025\n\n\n\n\n\n\n\nMixage Ambisonique dans REAPER - Introduction\n\n\n4 déc. 2024\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\n3 nov. 2024\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\n16 oct. 2024\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#dernières-musiques",
    "href": "index.html#dernières-musiques",
    "title": "Jean-Loup Pecquais",
    "section": "",
    "text": "Titre\n\n\nDate\n\n\n\n\n\n\n\n\n\nPompéi\n\n\n14 mars 2025\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "index.html#derniers-articles",
    "href": "index.html#derniers-articles",
    "title": "Jean-Loup Pecquais",
    "section": "",
    "text": "Titre\n\n\nDate\n\n\n\n\n\n\n\n\n\nComprendre les différents stratégies de spatialisation sonore\n\n\n3 juil. 2025\n\n\n\n\n\n\n\nMixage Ambisonique dans REAPER - Introduction\n\n\n4 déc. 2024\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\n3 nov. 2024\n\n\n\n\n\n\n\nRéseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare\n\n\n16 oct. 2024\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "training/initiation-3D-sonore/index.html",
    "href": "training/initiation-3D-sonore/index.html",
    "title": "Initiation à la 3D sonore",
    "section": "",
    "text": "Cette formation, intitulée “Initiation à la 3D Sonore,” est une immersion de cinq jours dans l’univers captivant de la création sonore spatialisée. Conçue par Jean-Marc Duchenne, elle est désormais proposée par le centre de formation Whiti Audio. Au cours de cette formation, vous explorerez l’évolution de la création sonore depuis le XXème siècle, des pionniers tels que Xenakis et Stockhausen jusqu’aux dernières avancées en réalité virtuelle et augmentée.\nLe programme complet de la formation englobe les bases de la perception audio spatiale, les formats de réalisation et de diffusion sonore, les techniques de capture sonore, les méthodes de traitement spatial, ainsi que les environnements logiciels et matériels pour la production immersive. Vous découvrirez les nuances de la diffusion en haute résolution, de l’ambisonique, du mixage orienté objets, du binaural et bien plus encore. Préparez-vous à repousser les limites de la perception sonore en vous inscrivant à la formation “Initiation à la 3D Sonore.” Explorez un monde sonore en trois dimensions et plongez dans l’avenir passionnant de la création sonore spatiale."
  },
  {
    "objectID": "training/informatique-audio/index.html",
    "href": "training/informatique-audio/index.html",
    "title": "Informatique audio",
    "section": "",
    "text": "Le cours “Informatique Audio”, dispensé sur trois jours, constitue une exploration approfondie de l’intersection entre l’informatique et les métiers du son. Dispensé aux étudiants deÒ première année de l’École Nationale Supérieure Louis Lumière, il a pour objectif de fournir aux étudiants une base solide dans ce domaine passionnant.\nCe cours commence par retracer l’origine de l’informatique, repartant de technologies simples pour expliquer la complexité des systèmes contemporains. Ensuite, les étudiants explorent les bases en architecture informatique, acquérant ainsi une compréhension des fondements des systèmes informatiques et de leur fonctionnement, essentielle pour maîtriser les outils audionumériques actuels.\nUne part significative du cours se consacre à l’étude des systèmes informatiques professionnels et dédiés au traitement de son en temps réel. Cette section inclut l’exploration de matériels propriétaires dédiés, de solutions généralistes et de systèmes embarqués, offrant aux étudiants un tour d’horizon de ces technologies cruciales pour les métiers du son.\nEnfin, le cours présente des études de cas réelles de déploiement de systèmes informatiques, que ce soit dans des studios d’enregistrement ou des événements live. Ces exemples concrets permettent aux étudiants de voir comment les principes théoriques s’appliquent dans la pratique, renforçant ainsi leur compréhension globale de l’informatique appliquée à audio."
  },
  {
    "objectID": "training/initiation-programmation/index.html",
    "href": "training/initiation-programmation/index.html",
    "title": "Initiation à la programmation",
    "section": "",
    "text": "Le cours « Initiation à la programmation pour les métiers du son » est destiné aux étudiants de deuxième année de l’ENS Louis Lumière. D’une durée de trois jours, il se concentre principalement sur des travaux pratiques.\nLes participants auront l’occasion d’apprendre un langage de programmation haut niveau et impératif, en commençant par la création de scripts pour le logiciel REAPER. Ce cours se termine par la réalisation de traitements sur le signal sonore directement exploitable dans un Digital Audio Workstation (DAW).\nAu terme de ces trois jours, les étudiants seront prêts à intégrer des éléments de programmation dans leur pratique professionnelle, leur ouvrant ainsi de nouvelles possibilités dans leurs futurs domaines d’expertise."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt1/index.html",
    "href": "posts/reseau-neuronne-deconvolution-pt1/index.html",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "",
    "text": "image générée par IA\nÀ l’heure où j’écris cet article, la simulation d’équipements “audiomusicaux” repose de plus en plus sur l’entraînement de réseaux de neurones 1, souvent grâce à l’apprentissage profond (deep learning) 2. Cette méthode est particulièrement séduisante, car elle ne nécessite aucune connaissance préalable sur l’équipement à modéliser. On parle alors de simulation “boîte noire” (black box) : seul le résultat final compte, peu importe comment il est produit.\nMe concernant, je suis intrigué par la capacité de cette technologie à fournir des simulations convaincantes d’amplificateurs pour guitare électrique. Plus particulièrement, la modélisation de l’étage d’amplification de puissance est cruciale pour remplacer un amplificateur sur scène sans perdre son effet sur le signal. Rappelons que les amplificateurs de guitare sont, par conception, particulièrement non linéaires, que ce soit dans leur réponse en fréquence ou dans l’ajout de distorsion harmonique.\nLe second aspect intéressant dans l’apprentissage profond est la possibilité de modéliser ses équipements. Voici alors la situation idéale : laissez votre ampli à lampe préféré à la maison, faites en une simple “emprunte” et emmener seulement une petite “boîte noire” numérique sur scène. Sur le papier, l’idée semble convaincante.\nLes difficultés commencent lorsque l’on souhaite réaliser ses propres captures. En règle générale, la procédure est similaire pour la plupart des fabricants de telles simulations : on émet un signal dans notre matériel, on enregistre la sortie du périphérique, puis on entraîne le réseau de neurones en lui injectant les deux signaux, soit sur un appareil spécifique, soit en ligne, sur une plateforme telle que Google Colab, par exemple.\nCependant, il faut bien distinguer plusieurs types d’équipements :\nDans le premier cas, l’acquisition pose moins de difficulté. On sort le signal de son interface audio, on récupère la sortie du périphérique dans une entrée en ligne ou un préamplificateur, et le tour est joué.\nDans le second cas, les choses se corsent. Un ampli de puissance à lampe possède un transformateur de sortie qui doit impérativement voir une charge particulière à ses bornes, sinon, on risque la défaillance de l’équipement. De plus, le niveau de sortie est tel qu’il est inimaginable de câbler la sortie d’un amplificateur de puissance dans une entrée en ligne. On a alors besoin d’un équipement qui va à la fois jouer le rôle de charge passive du haut-parleur (loadbox) et d’atténuateur vers un niveau en ligne.\nDans cette famille d’équipement, que nous appellerons simplement loadbox, on distingue deux catégories principales:\nCes dernières sont les plus “réalistes” par rapport au comportement d’un haut-parleur réel, qui, lui-même, ne possède pas une impédance d’entrée constante en fonction de la fréquence.\nSi les loadbox à charges purement résistives sont relativement abordables, les autres le sont beaucoup moins. C’est donc un investissement conséquent à rajouter en plus de son équipement à modéliser et de son équipement permettant de réaliser le traitement du signal à partir d’un modèle issu d’un apprentissage profond.\nQui plus est, cette approche tend à considérer le haut-parleur guitare comme un transducteur purement linéaire. Typiquement, une chaîne de traitement numérique “moderne” pour guitariste inclut une simulation par réseau de neurones pour la partie amplification et une opération de convolution pour la simulation du haut-parleur (convolution 4 réalisée avec la réponse impulsionnelle faite d’un haut-parleur). Rappelons ici que l’opération de convolution est une opération strictement linéaire. Cette méthode minimise ainsi le rôle du haut-parleur dans les non-linéarités introduites sur le signal.\nLa question est donc: comment pourrait-on faire mieux, ou, du moins, autrement ?\nLa méthode proposée ici est de ne pas court-circuiter le haut-parleur dans l’étape de la mesure. Ainsi, au lieu de relier l’amplificateur de puissance à une loadbox, ce dernier est normalement connecté à son/ses haut-parleur(s). Dès lors, on enregistre simplement ce qu’émet le haut-parleur à l’aide d’un microphone.\nDe prime à bord, on inclut donc les non-linéarités du haut-parleur et ses interactions avec l’amplificateur de puissance, mais aussi la réponse en fréquence du transducteur ! On perd alors la flexibilité de pouvoir changer “virtuellement” de haut-parleur en changeant de réponse impulsionnelle dans un moteur de convolution.\nLa solution proposée est donc de réaliser, en plus de la mesure nécessaire à l’entraînement du réseau de neurones, la réponse impulsionnelle du haut-parleur, puis de déconvoluer 5 cette réponse impulsionnelle des enregistrements réalisés en vue de l’apprentissage profond. Afin de ne pas altérer la réponse en fréquence de l’amplificateur, on réalise la réponse impulsionnelle avec un autre amplificateur, que l’on sait suffisamment linéaire.\nQuels sont les avantages de cette méthode ? Premièrement, il semble que l’on “extrait” ici plus d’information sur le comportement de l’amplificateur que l’on cherche à modéliser. Deuxièmement, un microphone de mesure et un amplificateur relativement “transparent” sont des équipements moins onéreux qu’une loadbox à charge variable.\nDans le prochain article, nous aborderons dans le détail le protocole de mesure ainsi que les enjeux entourant la déconvolution."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt1/index.html#footnotes",
    "href": "posts/reseau-neuronne-deconvolution-pt1/index.html#footnotes",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nEn apprentissage automatique (machine learning), un réseau de neurones (également appelé réseau de neurones artificiels ou réseau neuronal) est un modèle inspiré par la structure et le fonctionnement des réseaux neuronaux biologiques dans les cerveaux des animaux. Les réseaux de neurones artificiels sont utilisés pour diverses tâches, notamment la modélisation prédictive, le contrôle adaptatif et la résolution de problèmes en intelligence artificielle. Ils peuvent apprendre de l’expérience et tirer des conclusions à partir d’un ensemble complexe et apparemment non lié d’informations.↩︎\nL’apprentissage profond est une sous-catégorie de l’apprentissage automatique qui utilise des réseaux de neurones avec de nombreuses couches pour modéliser des structures de données complexes.↩︎\nL’impédance est une mesure de l’opposition d’un circuit électrique au passage d’un courant alternatif. Elle est une combinaison de la résistance et de la réactance.↩︎\nPour les lectrices et lecteurs moins aguerris en traitement du signal, l’opération de convolution est d’une importance majeure dans la matière. Voici un bon point de départ.↩︎\nLa déconvolution est une opération mathématique utilisée pour inverser les effets de la convolution, permettant de retrouver le signal d’origine à partir d’un signal convolué.↩︎"
  },
  {
    "objectID": "posts/formats-d-ecoute/index.html",
    "href": "posts/formats-d-ecoute/index.html",
    "title": "Comprendre les différents stratégies de spatialisation sonore",
    "section": "",
    "text": "Depuis début 2025, j’ai décidé d’utiliser ce site pour partager mes créations sonores et musicales (surtout musicales). La difficulté de cet exercice est que je ne travail plus en stéréo, et ceux depuis maintenant quelques années. La stéréo, si jamais le terme ne vous est pas familié, est un dispositif d’écoute, comprenant deux enceintes, placées de tel façon à former un triangle équilatéral avec l’auditeur. On créer ainsi une scène sonore frontale.\nIl est important de préciser que la stéréophonie repose sur un phénomène psycho-acoustique très important : le centre fantôme. Si un signal est émis sur les deux haut-parleurs avec la même intensité et la même phase [^Exactement au même instant du temps], alors le son semble venir d’une enceinte virtuelle située entre au milieu de la scène stéréophonique. De plus, si on altère le gain ou la phase d’un des canaux, on peut alors déplacer le son d’une enceinte à une autre.\nAlors pourquoi abandonner le mixage stéréophonique et vers me suis-je tourné ?\nUn mixage stéréophonique à deux défauts principaux :\n\nOn créer un mixage par rapport à une disposition de haut-parleurs particulières. Autrement dit, si l’on ne respecte pas le placement expliqué ci-dessus, ou si nous avons plus ou moins de haut-parleurs disponibles, nous risquons de ne pas correctement resituer notre mixage.\nLa stéréophonie n’offre qu’une scène sonore frontale. Il est parfois intéressant de pouvoir placer une source sonore arbitrairement autour de l’auditeur.\n\nPour palier à ces deux problèmes, il existe deux principales solutions:\n\nLes mixage orienté objet\nL’ambisonie\n\nLe mixage orienté objet, aujourd’hui synonyme, à tort, de Dolby Atmos, est un stratégie consistant à associer à chaque canaux audio d’un mixage (disons à chaque instrument ou élément d’un mixage pour simplifier), des métadonnées permettant de décrire la position (entre autre) de ces canaux dans l’espace. L’ensemble des deux forment un objet sonore, d’où le terme “mixage orienté objet”. Il n’y a donc pas de sommation des canaux. Cette étape de fabrication du mixage (donc de sommation) se fait chez l’auditeur directement, en fonction de son système de reproduction. Cette approche, très flexible sur la papier, est aussi très contraignante car le mixeur perd le contrôle de la sommation des éléments dans l’espace.\nL’ambisonie aborde le problème d’un autre point de vue, celui de la physique. L’ambisonie cherche à représenter le champ acoustique en un point. Cela nous permet de créer un mixage sans considérer le système d’écoute mais tout en conservant des points de sommations dans notre mixage. C’est vers ce format de mixage que je me tourne aujourd’hui.\nLes productions hébergées sur ce site sont généralement disponibles dans quatres formats différents : en binaural, UHJ stéréo, transaural et en ambisonie.\nL’ambisonie, comme expliqué ci-dessus, est le mixage original. Il peut, par exemple, être décodé vers un arrangement de haut-parleur arbitraire. Il est disponible pour qu’un utilisateur aggéri puisse écouter une des productions hébergée sur le site dans un format qui l’arrange. Par contre, l’ambisonie ne peut pas être écouté tel quel. Il doit nécessairement être décodé vers un format écoutable.\nL’UHJ stéréo est le format le plus proche de la stéréo conventionnelle. On peut donc l’écouter sur une paire de haut-parleurs mais sans effet d’entourement ou d’enveloppement. Pour en savoir plus sur cette technologie, je vous renvois vers sa page wikipédia. Comme pour la stéréophonie classique, l’UHJ ne se comporte pas très bien lorsqu’il est écouté au casque.\nLe binaural est un format d’écoute dit “immersif”. Il reproduit une scène sonore entourant l’auditeur. Cependant, le binaural doit être impérativement écouté au casque ou avec des écouteurs pour reproduire cet effet d’entourement.\nLe transaural, dernier format d’écoute disponible sur ce site, permet de d’étendre la scène sonore reproductible par un système stéréophonique. Sans qualifier ce format de proprement entourant, il produit tout de même une sensation de largeur assez surprenante. Il est parcontre indispensable de respecter scrupuleusement le placement des enceintes et de l’auditeur en forme de triangle équilatéral, sous peine de sacrifier les qualités spatiales du transaural. On notera que le transaural se comporte relativement correctement au casque.\nVoilà qui devrait normalement vous aiguiller un peu plus sur les formats d’écoutes disponiblent ici !"
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "",
    "text": "Image générée par IA\nDans l’article précédent, nous avions discuté d’un ensemble de problématiques entourant la création de modèles pour les réseaux de neurones dans le but de simuler des amplificateurs de guitare, ainsi que proposé une piste d’amélioration. Nous la résumerons de la façon suivante :\nVoyons maintenant la mise en pratique."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#liste-des-courses",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#liste-des-courses",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Liste des courses",
    "text": "Liste des courses\nL’amplificateur que nous souhaitons modéliser est l’amplificateur de puissance d’un Laney LionHeart L20T-410. Le microphone utilisé est un Behringer ECM8000. L’amplificateur utilisé pour réaliser la réponse impulsionnelle est un Drawmer CPA-50.\nPour assurer la simulation d’amplificateur, la solution choisie est le logiciel « AIDA-X ». Plusieurs raisons expliquent ce choix : AIDA-X est une solution open source. Elle offre une extension multi-plateforme et est accessible via la pédale MOD Dwarf. Elle repose sur RTNeural, un framework open source créé par Jatin Chowdhury.\nLe protocole de création de modèles pour AIDA-X est détaillé dans leur Google Colab. Le principe est relativement simple. On suit les étapes décrites dans le notebook, on récupère le fichier audio à lire au travers du périphérique à modéliser, on enregistre le résultat, puis on envoie les deux fichiers sur le notebook pour réaliser l’entraînement du réseau de neurones."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#mesure-de-la-réponse-impulsionnelle",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#mesure-de-la-réponse-impulsionnelle",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Mesure de la réponse impulsionnelle",
    "text": "Mesure de la réponse impulsionnelle\nLa réponse impulsionnelle du haut-parleur est réalisée grâce à REAPER. En effet, le plug-in ReaVerb permet de réaliser une déconvolution si le signal utilisé pour la mesure est un “sweep” logarithmique1. Pour plus d’information sur la procédure à suivre, voilà un lien vers un excellent article de Sound on Sound sur le sujet.\nLe signal sort du convertisseur vers un boîtier de réamping2 pour ensuite attaquer l’entrée de l’amplificateur Dawmer. Le haut-parleur du Laney est directement connecté sur ce dernier.\nUne fois tout cela réalisé, on se retrouve avec cette réponse impulsionnelle."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#enregistrement-des-empreintes",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#enregistrement-des-empreintes",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Enregistrement des empreintes",
    "text": "Enregistrement des empreintes\nL’amplificateur Laney possède un réglage de présence. Ce type de réglage contrôle en général un filtre passe-bas dans une boucle de rétroaction partant de la sortie de transformateur vers l’entrée de l’amplificateur de puissance.\nIl a donc été décidé de créer onze empreintes, une pour chacune des valeurs numériques indiquées par le potentiomètre de présence."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#problématique-de-la-déconvolution-des-empreintes",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#problématique-de-la-déconvolution-des-empreintes",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Problématique de la déconvolution des empreintes",
    "text": "Problématique de la déconvolution des empreintes\nNous voici donc dans le dur du sujet. Reprenons quelques bases de traitement du signal. Lorsque l’on étudie un système linéaire et invariant temporellement3, on sait que le signal d’entrée et le signal de sortie sont liés à la réponse impulsionnelle du système par l’opération de convolution.\n\\[\ny(t) = x(t) * h(t)\n\\]\nOù \\(x\\) est notre signal d’entré, \\(y\\) notre signal de sortie, et \\(h\\) la réponse impulsionnelle du système.\nNous savons qu’une des propriétés très intéressantes de la transformée de Fourier est de transformer une convolution temporelle en multiplication fréquentielle.\n\\[\nY(f) = X(f) \\times H(f)\n\\]\nOù \\(Y\\), \\(X\\) et \\(H\\) sont les transformée de Fourier respective de \\(y\\), \\(x\\) et \\(h\\).\n\n\n\n\n\n\nImportant\n\n\n\nN’oublions pas, \\(Y(f)\\), \\(X(f)\\) et \\(H(f)\\) sont des nombres complexes4.\n\n\nDès lors, si nous connaissons le signal d’entrée, le signal de sortie et que nous cherchons la réponse impulsionnelle, on trouve facilement que :\n\\[\nH(f) = \\frac{Y(f)}{X(f)}\n\\]\nSeulement, l’opération de division est problématique, puisque, si, pour certaines valeurs de \\(f\\), \\(X(f)\\) vaut zéro, \\(H(f)\\) tendra vers l’infini. Cela se traduira par l’apparition de “sifflantes” dans le signal audio déconvolué.\nUne solution classique au problème consiste d’abord à transformer notre expression précédente afin que le dénominateur de la fonction devienne un réel, puis à y ajouter une erreur pour limiter l’influence des valeurs de \\(X(f)\\) trop proches de zéro.\nOn commence donc par multiplier le dénominateur et le numérateur de notre expression de \\(H(f)\\) par le complexe conjugué de \\(X(f)\\), que nous écrirons \\(X^*(f)\\). On trouve donc :\n\\[\nH(f) = \\frac{Y(f).X^*(f)}{X^2(f)}\n\\]\nOn définit maintenant une erreur, soit constante \\(\\epsilon\\), soit variable en fonction de la fréquence \\(\\epsilon(f)\\). Il en découle alors :\n\\[\nH(f) = \\frac{Y(f).X^*(f)}{X^2(f)+\\epsilon^2(f)}\n\\]\nIl n’existe pas, à ma connaissance, de méthode classique pour déterminer \\(\\epsilon\\). Il est d’usage d’ajouter l’erreur la plus faible possible, afin de limiter l’approximation de l’opération de déconvolution. Dans la même logique, chercher une erreur variable en fonction de la fréquence est en général la meilleure approche.\n\n\n\n\n\n\nAstuce\n\n\n\nLa proposition d’une méthode pour approcher une valeur d’\\(\\epsilon\\) fera peut-être l’objet d’un article. Un jour.\n\n\nOn appelle cette méthode de déconvolution la déconvolution de Weiner."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#implémentation-de-la-déconvolution",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#implémentation-de-la-déconvolution",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Implémentation de la déconvolution",
    "text": "Implémentation de la déconvolution\nÀ ma connaissance, il n’existe pas d’outil de déconvolution gratuit n’imposant pas de contrainte sur le signal utilisé pour la mesure. Le plus simple fut donc d’implémenter l’algorithme de déconvolution en Python, en utilisant les librairies Numpy et SciPy.\nLe choix a été fait d’implémenter la déconvolution comme la convolution par la réponse impulsionnelle de l’inverse du spectre du signal d’entrée. Le formalisme mathématique peut ici nous éclairer. Nous avons écrit plus haut :\n\\[\nH(f) = \\frac{Y(f)}{X(f)}\n\\]\nNous pouvons tout aussi bien écrire :\n\\[\nH(f) = Y(f) \\times \\frac{1}{X(f)}\n\\]\nSoit, par transformée de Fourier inverse : \\[\nh(t) = y(t) * x_{inv}(t)\n\\]\nOù \\(TF(x_{inv}(t)) = \\frac{1}{X(f)}\\)\nRappelons-nous tout de même que nous souhaitons limiter l’apparition de sifflante par l’ajout d’une erreur. On pause alors la relation suivante :\n\\[\nTF(x_{inv}(t)) = \\frac{X^*(f)}{X^2(f)+\\epsilon^2(f)}\n\\]\nVoici une proposition d’implémentation en Python:\ndef invert_filter(impulse: np.ndarray, epsilon: np.ndarray = 0):\n    Impulse = np.fft.fft(impulse)\n    Kernel = np.conj(Impulse) / (Impulse*np.conj(Impulse)+np.power(epsilon,2))\n    return np.real(np.fft.ifft(Kernel))\nCette implémentation présuppose que le paramètre epsilon de la fonction est soit un nombre, soit une liste de même taille que le paramètre impulse. Le code suivant propose un exemple de l’utilisation de cette fonction :\nimport numpy as np\nfrom scipy import io, signal\n\n# Fonction de génération de \"l'inverse\" de la réponse impulsionnelle\ndef invert_filter(impulse: np.ndarray, epsilon: np.ndarray = 0):\n    Impulse = np.fft.fft(impulse)\n    Kernel = np.conj(Impulse) / (Impulse*np.conj(Impulse)+np.power(epsilon,2))\n    return np.real(np.fft.ifft(Kernel))\n\n# Conversion décibel vers linéaire\ndef db2a(x):\n    return np.power(10,x/20)\n\n# Définition de la fréquence d'échantillonnage de travail\nfs = 48000\n\n# On charge l'IR et l'empreinte\nimpulse_sr, impulse = io.wavfile.read('impulse_response.wav')\nmodel_sr, model = io.wavfile.read('model.wav')\n\n# On définie le niveau de bruit que l'on va rajouter pour la déconvolution. Ici, l'erreur est donc constante.\nnoise_floor = -100 # dB\n\n# la variable kernel stocke \"l'inverse\" de notre réponse impulsionnelle\nkernel = invert_filter(impulse,db2a(noise_floor))\n\n# La fonction lfilter permet de réaliser l'opération de convolution.\noutput = signal.lfilter(kernel,a=1,x=model)\n\n# Cette normalisation est recommandée par AIDA-X\n# https://mod.audio/modeling-amps-and-pedals-for-the-aida-x-plugin-best-practices/\noutput /= (np.max(output)*db2a(6))\n\n# On écrit la variable output dans un fichier\nio.wavfile.write('model_deconvolved.wav',fs,output)"
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#résultats",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#résultats",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Résultats",
    "text": "Résultats\nUne fois le script exécuté, on obtient notre fichier cible pour le réseau de neurones, où l’empreinte sonore du haut-parleur a été retirée.\nEn guise de comparaison, voici le même signal audio, avant et après déconvolution :\n\n\n\n\nDans le prochain article, nous regarderons comment implémenter ce modèle sur une carte Bela, puis nous prendrons le temps d’écouter le résultat."
  },
  {
    "objectID": "posts/reseau-neuronne-deconvolution-pt2/index.html#footnotes",
    "href": "posts/reseau-neuronne-deconvolution-pt2/index.html#footnotes",
    "title": "Réseau de neurones et déconvolution - Application à la simulation d’un amplificateur guitare",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nUn sweep est un signal constitué d’un son pur, mais dont la fréquence évolue dans le temps. L’objectif est de couvrir toutes les gammes de fréquences que l’on souhaite examiner. Dans le domaine de la mesure du matériel audio, il est recommandé d’utiliser un balayage logarithmique, où la fréquence augmente ou diminue progressivement selon un schéma logarithmique. Cette approche est avantageuse, car notre sensibilité aux différentes fréquences suit une progression logarithmique.↩︎\nUn boîtier de réamping adapte le signal de niveau ligne pour une entrée d’amplificateur pour guitare.↩︎\nVoir wikipedia.↩︎\nVoir wikipedia↩︎"
  },
  {
    "objectID": "music/2025-03-14 - pompei - pompei/index.html",
    "href": "music/2025-03-14 - pompei - pompei/index.html",
    "title": "Pompéi",
    "section": "",
    "text": "Please wait\n      \n    \n    \n    \n      \n      \n        \n        \n          \n        \n        \n          0:00\n          0:00"
  },
  {
    "objectID": "music/2025-03-14 - pompei - pompei/index.html#options-découte",
    "href": "music/2025-03-14 - pompei - pompei/index.html#options-découte",
    "title": "Pompéi",
    "section": "Options d’écoute",
    "text": "Options d’écoute\nQuel format écouter ? Rien de plus simple !\n\n\n\nStéréo UHJ\nDestiné à être réstitué sur une paire de haut-parleurs sans effet d’entourement. C’est (presque) votre stéréo habituelle !\n\n\n\nBinaural\nPour une écoute au casque avec un effet d’entourement.\n\n\n\nTransaural\nPour une paire de haut-parleurs. La scène sonore est considérablement élargie pour un système et un auditeur correctement positionnés\n\n\n\nAmbisonie\nContient le mixage original, avant décodage vers un format écoutable (stéréo et autre). Nécessite donc un décodeur externe et un utilisateur averti !\n\n\nPour les plus curieux, voici un article complet sur le sujet !"
  },
  {
    "objectID": "music.html",
    "href": "music.html",
    "title": "Musique",
    "section": "",
    "text": "Pompéi\n\n\nAlbum\n\n\n\nPompéi\n\n\n\nEnregistrement de reprises de Pink Floyd au studio Nyima\n\n\n\n\n\n14 mars 2025\n\n\nPompéi\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "posts/ambisonic-reaper-1/index.html",
    "href": "posts/ambisonic-reaper-1/index.html",
    "title": "Mixage Ambisonique dans REAPER - Introduction",
    "section": "",
    "text": "image générée par IA"
  },
  {
    "objectID": "posts/ambisonic-reaper-1/index.html#introduction",
    "href": "posts/ambisonic-reaper-1/index.html#introduction",
    "title": "Mixage Ambisonique dans REAPER - Introduction",
    "section": "Introduction",
    "text": "Introduction\n\nLa “révolution” de l’Atmos ?\nÀ l’heure où j’écris cet article, en début 2025, la spatialisation sonore dans la musique enregistrée prend le visage du Dolby Atmos. Trente ans plus tôt, c’était le 5.1 surround, puis le 7.1 et maintenant, la firme américaine nous propose un format de mixage orienté objet, permettant ainsi, théoriquement, de s’affranchir de la connaissance du système de diffusion lors de la phase de mixage.\nNe faisons pas durer le suspense plus longtemps, je fais partie des personnes tout à fait dubitatives devant l’engouement provoqué par cette technologie. En effet, je crois ses défauts assez nombreux. Prenons le temps d’en énumérer quelques-uns :\n\nUn mixage réalisé en Dolby Atmos ne peut être rendu, à ce jour, que sur certains systèmes de diffusion particuliers (5.1.2,7.1.4,9.1.6,etc.), tous ses systèmes ayant le mauvais goût d’être hétérogènes dans la répartition des haut-parleurs dans l’espace.\nLa loi de panoramique adopté par le Dolby Atmos emprunte beaucoup au Vector Based Amplitude Panning (ou VBAP). Bien que le VBAP possède un certain nombre de qualités (économe en ressources et relativement robuste face à des systèmes de diffusion non homogène), il possède malheureusement un inconvénient : il génère une variation de la perception de la largeur de la source en fonction de sa position. Cela s’entend particulièrement sur des mouvements.\nIl n’existe, à ce jour, pas de solution réellement satisfaisante pour le traitement des groupes d’objets. Quid de la compression de bus, de la compression parallèle et autres traitements aujourd’hui synonymes de certaines esthétiques de production ? Dolby, et les acteurs de l’industrie du son « immersif » répondent en général que l’amélioration des outils de spatialisation améliore également le démasquage (par la multiplication du nombre d’enceintes). Il s’agit d’un argument de techniciens, ne faisant pas la part des choses entre des traitements sonores de nécessité technique et ceux de nécessité esthétique.\nComme il n’existe plus de bus master en Atmos, la notion de mastering devient elle aussi bien compliquée. De plus, les résultats peuvent varier considérablement en fonction du format de restitution (haut-parleurs ou casque avec synthèse binaurale). Ici encore, aucune solution correcte n’est proposée, Dolby allant même jusqu’à proclamer que l’Atmos rend le mastering inutile.\nLa qualité sonore du rendu binaural des mixages Atmos est tout à fait discutable.\nLe prix de la chaîne de production Atmos est souvent prohibitif.\n\nMais tous ces défauts ne seraient rien sans la volonté de Dolby de faire passer sa technologie pour une révolution. Cette posture est particulièrement agaçante, car elle semble nier près de cent années de recherches sur le sujet. Car oui, la question de la spatialisation sonore de synthèse remonte presque aussi loin que l’invention de l’enregistrement sonore.\n\n\nL’alternative de l’ambisonie\nIl existe pourtant une technologie, dont la naissance remonte aux années soixante-dix, et qui proposait déjà des avantages similaires à l’Atmos : l’ambisonie. Mis au point par un génial ingénieur anglais, Michael Gerzon1, l’ambisonie permet, grâce à une technique de matriçage, de réaliser un mixage sans se soucier du système de diffusion (casque ou haut-parleurs, peu importe leurs nombres, etc.).\n\n\n\nMichael Gerzon - Prise de son ambisonique avec microphones discrets\n\n\nGerzon réalisa de nombreux enregistrements ambisoniques et fut aussi le concepteur des microphones Soundfield, marque aujourd’hui possédée par Rhodes. De santé fragile, il décéda dans les années quatre-vingt-dix.\nDepuis, de nombreuses recherches ont eu à cœur d’améliorer le procédé ambisonique. D’abord, par l’introduction de l’ambisonie d’ordre plus élevé (Higher Order Ambisonic - HOA), permettant ainsi un meilleur adressage des systèmes comportant beaucoup d’enceintes. Puis par l’élaboration de décodeurs optimisant le rendu ambisonique sur des systèmes non homogènes, comme le AllRound Ambisonic Decoder (AllRAD) et l’Energy Preserving Ambisonic Decoder (EPAD).\nIl existe aujourd’hui deux suites de plug-ins majeures permettant la création de mixages ambisonique:\n\nLa suite IEM\nLa suite SPARTA\n\n\n\n\n\n\n\n\n\n\nEnergyVisualizer de la suite IEM\n\n\n\n\n\n\n\n\n\nPlugins de la suite SPARTA\n\n\n\n\n\nToutes les deux sont distribuées gratuitement et sont open source. Cependant, un mixage ambisonique, surtout quand il utilise les ordres plus élevés, devient rapidement gourmand en nombre de canaux par piste. Un mixage réalisé à l’ordre 3 demande déjà seize canaux par pistes encodées. Il convient donc de trouver un logiciel de mixage permettant cette flexibilité.\nREAPER est une station de travail audio numérique développé par l’entreprise Cockos. Édité depuis 2005, ce logiciel a conquis une large communauté grâce à son grand nombre de fonctionnalités, ses grandes possibilités de personnalisation et un tarif terriblement concurrentiel. Dans notre cas d’usage, il est bon de dire que chaque piste, dans REAPER, supporte jusqu’à 128 canaux. Nous pouvons donc employer l’ambisonie d’ordre 10 (121 canaux) 2.\nCet article sert d’introduction à une série qui aura à cœur d’expliquer les éléments théoriques et pratiques du mixage ambisonique avec une attention particulière à proposer une méthode de travail simple, claire et qui ne remet pas en cause près de cent ans de pratiques techniques et surtout esthétiques dans nos métiers.\nLe prochain article traitera de la théorie entourant la pratique de l’ambisonique."
  },
  {
    "objectID": "posts/ambisonic-reaper-1/index.html#footnotes",
    "href": "posts/ambisonic-reaper-1/index.html#footnotes",
    "title": "Mixage Ambisonique dans REAPER - Introduction",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nGerzon est également le premier à proposer une approche de la réverbération artificiel employant une matrice de réinjection (Feedback Delay Network).↩︎\nEn 3D, le nombre de canaux d’un flux ambisonique ce calcul grâce à la formule suivante : \\(N_{ch} = (N+1)^2\\), où \\(N\\) est l’ordre ambisonique↩︎"
  },
  {
    "objectID": "training/pds-mix/index.html",
    "href": "training/pds-mix/index.html",
    "title": "Techniques de prises de son & de mixage",
    "section": "",
    "text": "Cette formation, dispensée par le centre de formation Transversal Studio à Vannes, est une opportunité pour les passionnés de musique et d’audio d’apprendre les techniques de production sonore en studio. Au cours de cette formation complète, les stagiaires découvriront l’ensemble de la chaîne audio, des nuances de l’acoustique à l’utilisation de microphones et d’équipements de pointe. Ils apprendront à capturer le son, en maîtrisant les subtilités du placement de microphones, des techniques stéréophoniques et de la gestion des réseaux casques.\nUne fois de l’autre côté de la vitre du studio, les stagiaires seront prêts à mettre en pratique leurs compétences nouvellement acquises. Ils sauront comment préparer une session, choisir les formats d’enregistrement et communiquer efficacement avec les musiciens. Ils perfectionneront également leurs compétences en mixage, en réalisant des balances et en ajoutant des effets audio pour créer des productions musicales de qualité professionnelle."
  },
  {
    "objectID": "training/spat-revolution/index.html",
    "href": "training/spat-revolution/index.html",
    "title": "SPAT Revolution",
    "section": "",
    "text": "Cette formation immersive sur le logiciel SPAT Revolution de FLUX:: est conçue pour les professionnels de l’audio souhaitant maîtriser la spatialisation sonore et créer des expériences audio immersives. Au début de la formation, les stagiaires découvriront les bases de SPAT Revolution, y compris l’installation et l’activation. Ensuite, ils plongeront dans les détails, explorant les différentes techniques de spatialisation, les différentes lois de panoramique, ou encore les différents paramètres de mixage et de perception sonores pour créer des scénarios sonores fascinants.\nAu cours de la formation, les stagiaires apprendront à automatiser des objets sonores, à configurer des systèmes de haut-parleurs, et à contrôler SPAT Revolution, via d’OSC, à l’aide d’iPad, et d’autres dispositifs. Ils développeront également des compétences essentielles pour la création de contenus immersifs, en travaillant sur des travaux pratiques concrets.\nCette formation ouvrira donc à ses stagiaires les portes de la spatialisation sonore et du logiciel SPAT Revolution."
  },
  {
    "objectID": "training/reaper/index.html",
    "href": "training/reaper/index.html",
    "title": "Production sonore avec REAPER",
    "section": "",
    "text": "Cette formation approfondie de REAPER, d’une durée de cinq jours, vise à familiariser les stagiaires avec ce logiciel puissant dédié à la production audio et à la composition musicale. Tout au long de cette formation, les stagiaires seront guidés à travers un apprentissage progressif pour qu’ils puissent maîtriser toutes les facettes de REAPER.\nDès le début, les participants seront plongés dans l’univers de REAPER, en commençant par l’installation, la configuration et la découverte de son interface. Ils apprendront à personnaliser REAPER pour qu’il corresponde parfaitement à leurs besoins spécifiques. Ensuite, ils plongeront dans l’enregistrement audio, en découvrant les techniques de gestion des prises, le routage audio et les compétences essentielles pour le montage audio. Ils auront également l’occasion d’acquérir des compétences avancées en matière d’édition audio, de manipulation de l’audio et de création de marqueurs.\nAu cours de la formation, les stagiaires exploreront également les aspects du mixage dans REAPER, en découvrant la bibliothèque d’effets, les automations, les groupes de pistes et bien d’autres fonctionnalités essentielles pour donner vie à leurs projets musicaux. Ils découvriront comment personnaliser encore davantage REAPER en configurant des raccourcis, des macros et en explorant les extensions et les scripts pour étendre leurs compétences de production audio. Cette formation permettra aux stagiaires de devenir des utilisateurs expérimentés de REAPER, capables de tirer pleinement parti de ce logiciel dans leurs projets musicaux et sonores."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projets",
    "section": "",
    "text": "Guide pratique des techniques du son\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReaVolution\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\nAucun article correspondant"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Cours & Formations",
    "section": "",
    "text": "Techniques de prises de son & de mixage\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitiation à la programmation\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSPAT Revolution\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformatique audio\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduction sonore avec REAPER\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitiation à la 3D sonore\n\n\n\n\n\n\nJean-Loup Pecquais\n\n\n\n\n\n\n\n\nAucun article correspondant"
  }
]